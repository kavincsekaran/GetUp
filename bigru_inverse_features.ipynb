{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/kchandrasekaran/virtualenvs/pytorch/bin/python\n",
    "#SBATCH --job-name=BiGRU_4_layers_inverse\n",
    "#SBATCH --output=BiGRU_4_layers.log\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -n 8\n",
    "#SBATCH --time=23:00:00\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH -C V100\n",
    "#SBATCH -p emmanuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import itertools\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import sklearn.metrics as met\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/X_train.txt\")\n",
    "X_test = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/X_test.txt\")\n",
    "Y_train = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/y_train.txt\")\n",
    "Y_test = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/y_test.txt\")\n",
    "train_uids = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/subject_id_train.txt\")\n",
    "test_uids = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/subject_id_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  5.,  6.,  7.,  8., 11., 14., 15., 16., 17., 19., 21.,\n",
       "       22., 23., 25., 26., 27., 28., 29., 30.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_labels_train = []\n",
    "for uuid in np.unique(train_uids):\n",
    "    #print(\"UUID: {}\".format(uuid))\n",
    "    uid_labels = Y_train[np.where(train_uids == uuid, True, False)]\n",
    "    groups = [(list(v), g) for g,v in itertools.groupby(uid_labels)]\n",
    "    processed=0\n",
    "    transition_y_hat=[]\n",
    "    #print(groups)\n",
    "    for group in groups:\n",
    "        cont=group[0]\n",
    "        g=group[1]\n",
    "        #print g\n",
    "        #print cont\n",
    "        cpd_label = np.zeros_like(cont)\n",
    "        cpd_label[0]=1\n",
    "        #print(cpd_label)\n",
    "        cpd_labels_train.append(cpd_label)\n",
    "cpd_labels_train = np.hstack(cpd_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/cpd_labels_train.txt\", cpd_labels_train, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_labels_test = []\n",
    "for uuid in np.unique(test_uids):\n",
    "    #print(\"UUID: {}\".format(uuid))\n",
    "    uid_labels = Y_test[np.where(test_uids == uuid, True, False)]\n",
    "    groups = [(list(v), g) for g,v in itertools.groupby(uid_labels)]\n",
    "    processed=0\n",
    "    transition_y_hat=[]\n",
    "    #print(groups)\n",
    "    for group in groups:\n",
    "        cont=group[0]\n",
    "        g=group[1]\n",
    "        #print g\n",
    "        #print cont\n",
    "        cpd_label = np.zeros_like(cont)\n",
    "        cpd_label[0]=1\n",
    "        #print(cpd_label)\n",
    "        cpd_labels_test.append(cpd_label)\n",
    "cpd_labels_test = np.hstack(cpd_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/cpd_labels_test.txt\", cpd_labels_test, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_labels_train = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/cpd_labels_train.txt\")\n",
    "cpd_labels_test = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/cpd_labels_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_test))\n",
    "Y = np.hstack((Y_train, Y_test))\n",
    "Y_cpd = np.hstack((cpd_labels_train, cpd_labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.genfromtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/features.txt\",dtype='str')\n",
    "label_names = np.genfromtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/activity_labels.txt\", dtype='str')\n",
    "label_names = [l[1] for l in label_names ]\n",
    "label_names\n",
    "\n",
    "transition_label_names=np.array(['ACTIVITIES', 'STAND_TO_SIT',\n",
    " 'SIT_TO_STAND',\n",
    " 'SIT_TO_LIE',\n",
    " 'LIE_TO_SIT',\n",
    " 'STAND_TO_LIE',\n",
    " 'LIE_TO_STAND'])\n",
    "\n",
    "activity_label_names=np.array(['WALKING',\n",
    " 'WALKING_UPSTAIRS',\n",
    " 'WALKING_DOWNSTAIRS',\n",
    " 'SITTING',\n",
    " 'STANDING',\n",
    " 'LAYING',\n",
    "'TRANSITIONS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out =nn.Parameter(dtype(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04357967, -0.00597022, -0.03505434, ..., -0.84155851,\n",
       "         0.17991281, -0.05171842],\n",
       "       [ 0.03948004, -0.00213128, -0.02906736, ..., -0.8450924 ,\n",
       "         0.18026111, -0.04743634],\n",
       "       [ 0.03997778, -0.00515272, -0.02265071, ..., -0.84923013,\n",
       "         0.18060956, -0.04227136],\n",
       "       ...,\n",
       "       [ 0.03745094, -0.00272442,  0.02100941, ..., -0.77956634,\n",
       "         0.24912145,  0.04707077],\n",
       "       [ 0.04401105, -0.00453578, -0.0512422 , ..., -0.78560327,\n",
       "         0.24640867,  0.03170003],\n",
       "       [ 0.06895376,  0.00181032, -0.08032343, ..., -0.78369253,\n",
       "         0.24678499,  0.04298129]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transition_train=X[:int(len(X)*0.6)]\n",
    "X_transition_validation=X[int(len(X)*0.6):int(len(X)*0.8)]\n",
    "X_transition_test=X[int(len(X)*0.8):]\n",
    "Y_transition_train=np.where(Y[:int(len(Y)*0.6)] > 6, Y[:int(len(Y)*0.6)], 0)\n",
    "Y_transition_validation=np.where(Y[int(len(Y)*0.6):int(len(X)*0.8)] > 6, Y[int(len(Y)*0.6):int(len(X)*0.8)], 0)\n",
    "Y_transition_test=np.where(Y[int(len(Y)*0.8):] > 6, Y[int(len(Y)*0.8):], 0)\n",
    "\n",
    "X_activities_train=X[:int(len(X)*0.6)]\n",
    "X_activities_validation=X[int(len(X)*0.6):int(len(X)*0.8)]\n",
    "X_activities_test=X[int(len(X)*0.8):]\n",
    "Y_activities_train=np.where(Y[:int(len(Y)*0.6)] < 7, Y[:int(len(Y)*0.6)], 0)\n",
    "Y_activities_validation=np.where(Y[int(len(Y)*0.6):int(len(X)*0.8)] < 7, Y[int(len(Y)*0.6):int(len(X)*0.8)], 0)\n",
    "Y_activities_test=np.where(Y[int(len(Y)*0.8):] < 7, Y[int(len(Y)*0.8):], 0)\n",
    "\n",
    "train_uid = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/subject_id_train.txt\")\n",
    "test_uid = np.loadtxt(\"/home/kavin/Silo/storage/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/subject_id_test.txt\")\n",
    "user_ids = np.hstack((train_uid, test_uid))\n",
    "rand_uid=[np.random.choice(np.unique(user_ids), len(np.unique(user_ids)), replace=False) for _ in range(5)]\n",
    "\n",
    "def one_hot(y, labels):\n",
    "    Y_onehot=[]\n",
    "    for l in y:\n",
    "        empty_label=np.zeros(len(labels))\n",
    "        empty_label[labels.index(l)]=1.\n",
    "        Y_onehot.append(empty_label)\n",
    "    return(np.vstack(Y_onehot))\n",
    "\n",
    "\n",
    "def get_metrics(target, output):\n",
    "        \n",
    "        pred = np.round(output)\n",
    "        \n",
    "        tp = np.sum(((pred + target) == 2).astype(float), axis=0)\n",
    "        fp = np.sum(((pred - target) == 1).astype(float), axis=0)\n",
    "        fn = np.sum(((pred - target) == -1).astype(float), axis=0)\n",
    "        tn = np.sum(((pred + target) == 0).astype(float), axis=0)\n",
    "\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "        try:\n",
    "            prec = tp / (tp + fp)\n",
    "        except ZeroDivisionError:\n",
    "            prec = 0.0\n",
    "        try:\n",
    "            rec = tp / (tp + fn)\n",
    "        except ZeroDivisionError:\n",
    "            rec = 0.0\n",
    "        try:\n",
    "            specificity = tn / (tn + fp)\n",
    "        except ZeroDivisionError:\n",
    "            specificity = 0.0\n",
    "\n",
    "\n",
    "        try:\n",
    "            f1=2.*((prec*rec)/(prec+rec))\n",
    "        except ZeroDivisionError:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        acc[acc != acc] = 0.\n",
    "        prec[prec != prec] = 0.\n",
    "        rec[rec != rec] = 0.\n",
    "        specificity[specificity != specificity] = 0.\n",
    "        f1[f1 != f1] = 0.\n",
    "        \n",
    "        balanced_accuracy = (rec + specificity) / 2.\n",
    "        \n",
    "        err_rate = np.subtract(1., acc)\n",
    "        f1_micro, f1_macro, f1_weight, log_ls, roc = [], [], [], [], []\n",
    "        for idx in range(target.shape[1]):\n",
    "            y_test=target[:,idx]\n",
    "            y_pred=pred[:,idx]\n",
    "            \n",
    "            f1_micro.append(f1_score(y_test, y_pred, average= 'micro'))\n",
    "            f1_macro.append(f1_score(y_test, y_pred, average= 'macro'))\n",
    "            f1_weight.append(f1_score(y_test, y_pred, average= 'weighted'))\n",
    "            log_ls.append(log_loss(y_test, y_pred, labels=[0., 1.]))\n",
    "            try:\n",
    "                roc.append(roc_auc_score(y_test, output[:, idx]))\n",
    "            except ValueError:\n",
    "                roc.append(np.nan)\n",
    "            \n",
    "        return (balanced_accuracy, acc, err_rate, prec, rec, specificity, f1, tp, fp, fn, tn, np.array(f1_micro), np.array(f1_macro), np.array(f1_weight), np.array(log_ls), np.array(roc))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "bidirectional = True\n",
    "if bidirectional:\n",
    "    num_directions = 2\n",
    "else:\n",
    "    num_directions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_GRU(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Custom_GRU, self).__init__()\n",
    "        self.input_size = config[\"input_dim\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_layers = config[\"num_layers\"] \n",
    "        self.output_size = config[\"output_dim\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.num_epochs = config[\"num_epochs\"]\n",
    "        self.num_directions = config[\"num_directions\"]\n",
    "        self.attention_setting = config[\"attention\"]\n",
    "        self.weight_decay = config[\"optimizer_decay\"]\n",
    "        \n",
    "        if config[\"num_directions\"] == 1:\n",
    "            bidirectional = False\n",
    "        elif config[\"num_directions\"] == 2:\n",
    "            bidirectional = True\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=self.input_size, num_layers= self.num_layers, hidden_size=self.hidden_size, \n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=0.1).cuda()\n",
    "        if(self.attention_setting == True):\n",
    "            self.linear = nn.Linear(self.hidden_size*self.num_directions, self.output_size)\n",
    "            self.attention_weights = new_parameter(1, self.hidden_size*self.num_directions)\n",
    "        else:\n",
    "            self.linear = nn.Linear(self.hidden_size*self.num_directions, self.output_size)\n",
    "        self.act = nn.Softmax()\n",
    "        self.loss_func = F.binary_cross_entropy\n",
    "        \n",
    "        #self.act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        if(self.attention_setting == True):\n",
    "            if(self.num_directions == 2):\n",
    "                H = torch.cat((hidden[0], hidden[1]), 1)\n",
    "            else:\n",
    "                H = hidden\n",
    "            M = torch.tanh(H.squeeze(0))\n",
    "            alpha = self.act(torch.mm( M, torch.transpose(self.attention_weights, 1, 0)))\n",
    "            r = torch.mul(H , alpha.expand_as(H))\n",
    "            h_wa = torch.tanh(r)\n",
    "            pred_out = self.act(self.linear(h_wa))\n",
    "        else:\n",
    "            pred_out = self.act(self.linear(pred).T).unsqueeze(0).cuda()\n",
    "        \n",
    "        return pred_out\n",
    "\n",
    "    def train_gru(self, uids, exids, train_inp, train_out):\n",
    "        #predictions = []\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        #attention_optimizer = torch.optim.Adam([self.attention_weights], lr=self.learning_rate)\n",
    "        \n",
    "        #loss_func = nn.L1Loss()\n",
    "        #attn_loss_fn = F.l1_loss\n",
    "        train_loss = []\n",
    "        for t in range(self.num_epochs):\n",
    "            ids, idxs = np.unique(exids, return_index=True)\n",
    "            for id_idx in range(len(ids)):\n",
    "                uid = ids[id_idx]\n",
    "                hidden = None\n",
    "                start_idx = idxs[int(id_idx)]\n",
    "                if(id_idx+1==len(ids)):\n",
    "                    end_idx = len(train_inp)\n",
    "                else:\n",
    "                    end_idx = idxs[int(id_idx+1)]\n",
    "                inp = Variable(torch.from_numpy(train_inp[start_idx:end_idx, :].reshape((end_idx-start_idx, -1, self.input_size))).type(dtype), requires_grad=True)\n",
    "                out = Variable(torch.from_numpy(train_out[start_idx:end_idx, :].reshape((end_idx-start_idx, self.output_size))).type(dtype))\n",
    "\n",
    "                pred = self.forward(inp)\n",
    "                optimizer.zero_grad()\n",
    "                #attention_optimizer.zero_grad()\n",
    "                #predictions.append(pred.data.cpu().numpy())\n",
    "                loss = self.loss_func(pred, out)\n",
    "                #attn_loss = attn_loss_fn(self.attention_weights, torch.zeros_like(self.attention_weights))\n",
    "                train_loss.append(loss.item())\n",
    "                if t%100==0:\n",
    "                    print(t, loss.item())\n",
    "                loss.backward()\n",
    "                #attn_loss.backward()\n",
    "                optimizer.step()\n",
    "                #attention_optimizer.step()\n",
    "                del inp\n",
    "                del out\n",
    "                del pred\n",
    "                del loss\n",
    "        return train_loss\n",
    "    \n",
    "    def test_gru(self, uids, exids, test_inp, test_act_out, test_out):\n",
    "        ids, idxs = np.unique(exids, return_index=True)\n",
    "        fold_metrics = []\n",
    "        for id_idx in range(len(ids)):\n",
    "            try:\n",
    "                uid = ids[id_idx]\n",
    "                hidden = None\n",
    "                start_idx = idxs[int(id_idx)]\n",
    "                if(id_idx+1==len(ids)):\n",
    "                    end_idx = len(test_inp)\n",
    "                else:\n",
    "                    end_idx = idxs[int(id_idx+1)]\n",
    "                print(uid, start_idx, end_idx, test_inp[start_idx:end_idx, :].shape)\n",
    "                t_inp = Variable(torch.Tensor(test_inp[start_idx:end_idx, :].reshape((end_idx-start_idx, -1, self.input_size))).type(dtype), requires_grad=True)\n",
    "                t_out = Variable(torch.Tensor(test_out[start_idx:end_idx, :].reshape((end_idx-start_idx, self.output_size))).type(dtype))\n",
    "                pred_t = self.forward(t_inp)\n",
    "                test_loss = self.loss_func(pred_t, t_out)\n",
    "                pred_numpy = pred_t.squeeze().data.cpu().numpy()\n",
    "                fold_metrics.append(pred_numpy)\n",
    "            except:\n",
    "                pass\n",
    "        return fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers, output_size):\n",
    "        super(Custom_GRU, self).__init__()\n",
    "        self.output_size=output_size\n",
    "        self.rnn = nn.GRU(input_size=input_size, num_layers= num_layers, hidden_size=hidden_size, \n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=0.1).cuda()\n",
    "        self.linear = nn.Linear(hidden_size*num_directions, output_size)\n",
    "        self.act = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        pred = self.act(torch.permute(self.linear(pred))).view(pred.data.shape[0], self.output_size).cuda()\n",
    "        return pred\n",
    "\n",
    "def train_gru(config, train_inp, train_out, test_inp, test_out):\n",
    "    r= Custom_GRU(config[\"input_dim\"], config[\"hidden_size\"], config[\"num_layers\"], config[\"output_dim\"]).to(device)\n",
    "    predictions = []\n",
    "    optimizer = torch.optim.Adam(r.parameters(), lr=config[\"learning_rate\"])\n",
    "    #loss_func = nn.L1Loss()\n",
    "    loss_func = F.binary_cross_entropy\n",
    "\n",
    "    for t in range(config[\"num_epochs\"]):\n",
    "        hidden = None\n",
    "        inp = Variable(torch.from_numpy(train_inp.reshape((train_inp.shape[0], -1, config[\"input_dim\"]))).type(dtype), requires_grad=True)\n",
    "        out = Variable(torch.from_numpy(train_out.reshape((train_inp.shape[0], config[\"output_dim\"]))).type(dtype))\n",
    "    \n",
    "        pred = r(inp)\n",
    "        optimizer.zero_grad()\n",
    "        predictions.append(pred.data.cpu().numpy())\n",
    "        loss = loss_func(pred, out)\n",
    "        if t%100==0:\n",
    "            print(t, loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    t_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 561))).type(dtype), requires_grad=True)\n",
    "    pred_t = r(t_inp)\n",
    "    pred_numpy = pred_t.data.cpu().numpy()\n",
    "    return pred_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    transition_act=np.where(np.argmax(pred_numpy, axis=1)==6, True, False)\n",
    "    print(transition_act)\n",
    "    groups = [(list(v), g) for g,v in itertools.groupby(transition_act)]\n",
    "    processed=0\n",
    "    transition_y_hat=[]\n",
    "    #print(groups)\n",
    "    for group in groups:\n",
    "        cont=group[0]\n",
    "        g=group[1]\n",
    "        trans_pred=0\n",
    "        if g:\n",
    "            if(len(cont)>0):\n",
    "                prev_act=np.argmax(pred_numpy[processed-1])+1\n",
    "                next_act=np.argmax(pred_numpy[processed+len(cont)])+1\n",
    "                if(prev_act==5 and next_act==4):\n",
    "                    trans_pred=7\n",
    "                elif(prev_act==5 and next_act==6):\n",
    "                    trans_pred=11\n",
    "                elif(prev_act==4 and next_act==5):\n",
    "                    trans_pred=8\n",
    "                elif(prev_act==4 and next_act==6):\n",
    "                    trans_pred=9\n",
    "                elif(prev_act==6 and next_act==4):\n",
    "                    trans_pred=10\n",
    "                elif(prev_act==6 and next_act==5):\n",
    "                    #print(\"class 12 should be predicted\")\n",
    "                    trans_pred=12\n",
    "                elif(prev_act==6 and next_act==1):\n",
    "                    trans_pred=12\n",
    "                else:\n",
    "                    print(\"Unexpected Combination. Prev{}. Next{}\".format(prev_act, next_act))\n",
    "                processed+=len(cont)\n",
    "            else:\n",
    "                processed+=len(cont)\n",
    "        else:\n",
    "            processed+=len(cont)\n",
    "        transition_y_hat.append(np.ones_like(cont)*trans_pred)\n",
    "    transition_y_hat = np.hstack(transition_y_hat)\n",
    "    \n",
    "    pred_one_hot = one_hot(transition_y_hat, [0, 7, 8, 9, 10, 11, 12])\n",
    "    \n",
    "    results_metrics = get_metrics(test_out, pred_one_hot)\n",
    "    results_conf_mat = confusion_matrix(np.argmax(test_out, axis=1)+1, np.argmax(pred_one_hot, axis=1)+1)\n",
    "    metric_names = np.array([\"CV\", \"Balanced Accuracy\", \"Accuracy\", \"Error Rate\", \"Precision\",\"Recall\",\"Specificity\", \"F1\", \"TP\",\"FP\",\"FN\",\"TN\", \"Micro F1\",\"Macro F1\",\"Weighted F1\",\"Log-Loss\",\"ROC AUC\"])\n",
    "    results = np.hstack((metric_names.reshape(-1, 1), np.vstack((transition_label_names, np.vstack(results_metrics)))))\n",
    "    \n",
    "    filename= config[\"result_filename\"]\n",
    "    with open(\"{}.csv\".format(filename), 'a') as f:\n",
    "        pd.DataFrame(results).to_csv(f, header=False)\n",
    "    with open(\"{}_conf_matrix.csv\".format(filename), 'a') as f:    \n",
    "        pd.DataFrame(np.hstack((transition_label_names.reshape(-1, 1), results_conf_mat))).to_csv(f, header=False)\n",
    "    plt.subplots(figsize=(20,15))\n",
    "    sns.set(font_scale = 1.8)\n",
    "    s=sns.heatmap(results_conf_mat.astype(int), annot=True, annot_kws={\"size\": 20}, cmap=\"YlGnBu\", fmt='d', xticklabels=transition_label_names, yticklabels=transition_label_names)\n",
    "    title=\"Transition Learning\"\n",
    "    s.set_title(title)\n",
    "    \n",
    "    filename_modifier = 0\n",
    "    while(os.path.exists(filename+\"_\"+str(filename_modifier)+\".png\")):\n",
    "        filename_modifier+=1\n",
    "    fig_fname=filename+\"_\"+str(filename_modifier)+\".png\"\n",
    "    s.get_figure().savefig(fig_fname, dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavin/virtualenvs/pytorch/local/lib/python2.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/kavin/virtualenvs/pytorch/lib/python2.7/site-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exp_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-efc20177b116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustom_GRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_act_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_act_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_act_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_act_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trans_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_ids' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes=[512]\n",
    "learning_rates=[1e-3]\n",
    "epochs=[100]\n",
    "directions = [2]\n",
    "layers=[1]\n",
    "attention = [True]\n",
    "optimizer_decay = [0]\n",
    "\n",
    "hyperparameters = [hidden_layer_sizes, directions, layers, epochs, learning_rates, attention, optimizer_decay]\n",
    "all_parameter_combinations=list(itertools.product(*hyperparameters))\n",
    "costs=[]\n",
    "\n",
    "for parameter_combo in all_parameter_combinations:\n",
    "    config = {}\n",
    "    config[\"input_dim\"] = 6\n",
    "    config[\"hidden_size\"] = parameter_combo[0]\n",
    "    config[\"num_directions\"] = parameter_combo[1]\n",
    "    config[\"num_layers\"] = parameter_combo[2]\n",
    "    config[\"output_dim\"] = 7\n",
    "    config[\"num_epochs\"] = parameter_combo[3]\n",
    "    config[\"learning_rate\"] = parameter_combo[4]\n",
    "    config[\"attention\"] = parameter_combo[5]\n",
    "    config[\"optimizer_decay\"] = parameter_combo[6]\n",
    "    if(config[\"attention\"] == True):\n",
    "        config[\"result_filename\"] = \"results/cpd/W_Decay_{}_Inverse_Transition_classification_w_attention_results_BiGRU_{}_directions_{}_layers_{}_lr_{}_units_{}_epochs\".format(config[\"optimizer_decay\"], config[\"num_directions\"], config[\"num_layers\"], config[\"learning_rate\"], config[\"hidden_size\"], config[\"num_epochs\"])\n",
    "    else:\n",
    "        config[\"result_filename\"] = \"results/cpd/W_Decay_{}_Inverse_Transition_classification_results_BiGRU_{}_directions_{}_layers_{}_lr_{}_units_{}_epochs\".format(config[\"optimizer_decay\"], config[\"num_directions\"], config[\"num_layers\"], config[\"learning_rate\"], config[\"hidden_size\"], config[\"num_epochs\"])\n",
    "    for cv_idx, cv_fold in enumerate(rand_uid):\n",
    "        train_ids, val_ids, test_ids = cv_fold[:int(0.6*len(cv_fold))], cv_fold[int(0.6*len(cv_fold)):int(0.8*len(cv_fold)):], cv_fold[int(0.8*len(cv_fold)):]\n",
    "        train_idx = np.isin(user_ids, train_ids)\n",
    "        val_idx = np.isin(user_ids, val_ids)\n",
    "        test_idx = np.isin(user_ids, test_ids)\n",
    "\n",
    "        X_activity_train=X[train_idx]\n",
    "        X_activity_validation=X[val_idx]\n",
    "        X_activity_test=X[test_idx]\n",
    "        Y_activity_train=np.where(Y[train_idx] <= 6, Y[train_idx], 0)\n",
    "        Y_activity_validation=np.where(Y[val_idx] <= 6, Y[val_idx], 0)\n",
    "        Y_activity_test=np.where(Y[test_idx] <= 6, Y[test_idx], 0)\n",
    "\n",
    "        Y_transition_train=np.where(Y[train_idx] > 6, Y[train_idx], 0)\n",
    "        Y_transition_validation=np.where(Y[val_idx] > 6, Y[val_idx], 0)\n",
    "        Y_transition_test=np.where(Y[test_idx] > 6, Y[test_idx], 0)\n",
    "\n",
    "        train_act_inp, train_act_out = X_activity_train, one_hot(Y_activity_train, [1, 2, 3, 4, 5, 6, 0])\n",
    "        val_act_inp, val_act_out = X_activity_validation, one_hot(Y_activity_validation, [1, 2, 3, 4, 5, 6, 0])\n",
    "        test_act_inp, test_act_out = X_activity_test, one_hot(Y_activity_test, [1, 2, 3, 4, 5, 6, 0])\n",
    "\n",
    "        train_trans_inp, train_trans_out = X_activity_train, one_hot(Y_transition_train, [0,7,8,9,10,11,12])\n",
    "        val_trans_inp, val_trans_out = X_activity_validation, one_hot(Y_transition_validation, [0,7,8,9,10,11,12])\n",
    "        test_trans_inp, test_trans_out = X_activity_test, one_hot(Y_transition_test, [0,7,8,9,10,11,12])\n",
    "\n",
    "        model = Custom_GRU(config).to(device)\n",
    "        train_loss = model.train_gru(user_ids[train_idx], exp_ids[train_idx], train_act_inp, train_act_out)\n",
    "        probs = model.test_gru(user_ids[val_idx], exp_ids[val_idx],val_act_inp, val_act_out, val_trans_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavin/virtualenvs/pytorch/local/lib/python2.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-73afd4a34b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtest_trans_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_trans_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_transition_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_transition_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_act_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_act_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_act_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trans_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-57c5744f2583>\u001b[0m in \u001b[0;36mtrain_gru\u001b[0;34m(config, train_inp, train_out, test_inp, test_out)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/virtualenvs/pytorch/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-57c5744f2583>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"input_dim\"] = 561\n",
    "config[\"hidden_size\"] = 1024 \n",
    "config[\"num_layers\"] = 1\n",
    "config[\"output_dim\"] = 7\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"learning_rate\"] = 1e-4\n",
    "config[\"result_filename\"] = \"CPD_{}_layers_{}_lr_{}_units_{}_epochs\".format(config[\"num_layers\"], config[\"learning_rate\"], config[\"hidden_size\"], config[\"num_epochs\"])\n",
    "\n",
    "for cv_idx, cv_fold in enumerate(rand_uid):\n",
    "    train_ids, val_ids, test_ids = cv_fold[:int(0.6*len(cv_fold))], cv_fold[int(0.6*len(cv_fold)):int(0.8*len(cv_fold)):], cv_fold[int(0.8*len(cv_fold)):]\n",
    "\n",
    "    train_idx = np.isin(user_ids, train_ids)\n",
    "    val_idx = np.isin(user_ids, val_ids)\n",
    "    test_idx = np.isin(user_ids, test_ids)\n",
    "    \n",
    "    X_activity_train=X[train_idx]\n",
    "    X_activity_validation=X[val_idx]\n",
    "    X_activity_test=X[test_idx]\n",
    "    Y_activity_train=np.where(Y[train_idx] <= 6, Y[train_idx], 0)\n",
    "    Y_activity_validation=np.where(Y[val_idx] <= 6, Y[val_idx], 0)\n",
    "    Y_activity_test=np.where(Y[test_idx] <= 6, Y[test_idx], 0)\n",
    "    \n",
    "    X_transition_train=X[train_idx]\n",
    "    X_transition_validation=X[val_idx]\n",
    "    X_transition_test=X[test_idx]\n",
    "    Y_transition_train=np.where(Y[train_idx] > 6, Y[train_idx], 0)\n",
    "    Y_transition_validation=np.where(Y[val_idx] > 6, Y[val_idx], 0)\n",
    "    Y_transition_test=np.where(Y[test_idx] > 6, Y[test_idx], 0)\n",
    "        \n",
    "    train_act_inp, train_act_out = X_activity_train, one_hot(Y_activity_train, [1, 2, 3, 4, 5, 6, 0])\n",
    "    val_act_inp, val_act_out = X_activity_validation, one_hot(Y_activity_validation, [1, 2, 3, 4, 5, 6, 0])\n",
    "    test_act_inp, test_act_out = X_activity_test, one_hot(Y_activity_test, [1, 2, 3, 4, 5, 6, 0])\n",
    "    \n",
    "    train_trans_inp, train_trans_out = X_transition_train, one_hot(Y_transition_train, [0,7,8,9,10,11,12])\n",
    "    val_trans_inp, val_trans_out = X_transition_validation, one_hot(Y_transition_validation, [0,7,8,9,10,11,12])\n",
    "    test_trans_inp, test_trans_out = X_transition_test, one_hot(Y_transition_test, [0,7,8,9,10,11,12])\n",
    "\n",
    "    act_probs = train_gru(config, train_act_inp, train_act_out, val_act_inp, val_trans_out)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.001, 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.001, 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.001, 0.   ],\n",
       "       ...,\n",
       "       [0.001, 0.001, 0.001, 0.   , 0.   , 0.   ],\n",
       "       [0.001, 0.001, 0.001, 0.   , 0.   , 0.   ],\n",
       "       [0.001, 0.001, 0.001, 0.   , 0.   , 0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(act_probs, 3)[:,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999946, 1.        , 1.0000004 , 0.9999991 , 1.0000007 ,\n",
       "       0.9999995 , 0.999999  ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(act_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07389315, 0.14769669, 0.0411515 , ..., 0.617333  , 0.16519511,\n",
       "        0.29530764],\n",
       "       [0.11910743, 0.12952448, 0.03114143, ..., 0.7026927 , 0.17746677,\n",
       "        0.13058355],\n",
       "       [0.22940716, 0.11094632, 0.04385746, ..., 0.5168176 , 0.13071676,\n",
       "        0.09855206],\n",
       "       ...,\n",
       "       [0.69182867, 1.203381  , 0.9907683 , ..., 0.01458044, 0.01255452,\n",
       "        0.45720252],\n",
       "       [0.8060233 , 1.0149624 , 0.83399075, ..., 0.01547358, 0.01568432,\n",
       "        0.4173171 ],\n",
       "       [0.9808758 , 1.2472504 , 0.83268404, ..., 0.0145801 , 0.01337154,\n",
       "        0.4567958 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.multiply(act_probs, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
