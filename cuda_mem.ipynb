{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import itertools\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import sklearn.metrics as met\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p= np.array([[0.2, 0.4, 0.6],\n",
    "        [0.5, 0.3, 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/X_train.txt\")\n",
    "X_test = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/X_test.txt\")\n",
    "Y_train = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/y_train.txt\")\n",
    "Y_test = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/y_test.txt\")\n",
    "\n",
    "X = np.vstack((X_train, X_test))\n",
    "Y = np.hstack((Y_train, Y_test))\n",
    "\n",
    "feature_names = np.genfromtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/features.txt\",dtype='str')\n",
    "label_names = np.genfromtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/activity_labels.txt\", dtype='str')\n",
    "label_names = [l[1] for l in label_names ]\n",
    "label_names\n",
    "\n",
    "transition_label_names=np.array(['ACTIVITIES', 'STAND_TO_SIT',\n",
    " 'SIT_TO_STAND',\n",
    " 'SIT_TO_LIE',\n",
    " 'LIE_TO_SIT',\n",
    " 'STAND_TO_LIE',\n",
    " 'LIE_TO_STAND'])\n",
    "\n",
    "activity_label_names=np.array(['WALKING',\n",
    " 'WALKING_UPSTAIRS',\n",
    " 'WALKING_DOWNSTAIRS',\n",
    " 'SITTING',\n",
    " 'STANDING',\n",
    " 'LAYING',\n",
    "'TRANSITIONS'])\n",
    "\n",
    "X_transition_train=X[:int(len(X)*0.6)]\n",
    "X_transition_validation=X[int(len(X)*0.6):int(len(X)*0.8)]\n",
    "X_transition_test=X[int(len(X)*0.8):]\n",
    "Y_transition_train=np.where(Y[:int(len(Y)*0.6)] > 6, Y[:int(len(Y)*0.6)], 0)\n",
    "Y_transition_validation=np.where(Y[int(len(Y)*0.6):int(len(X)*0.8)] > 6, Y[int(len(Y)*0.6):int(len(X)*0.8)], 0)\n",
    "Y_transition_test=np.where(Y[int(len(Y)*0.8):] > 6, Y[int(len(Y)*0.8):], 0)\n",
    "\n",
    "X_activities_train=X[:int(len(X)*0.6)]\n",
    "X_activities_validation=X[int(len(X)*0.6):int(len(X)*0.8)]\n",
    "X_activities_test=X[int(len(X)*0.8):]\n",
    "Y_activities_train=np.where(Y[:int(len(Y)*0.6)] < 7, Y[:int(len(Y)*0.6)], 0)\n",
    "Y_activities_validation=np.where(Y[int(len(Y)*0.6):int(len(X)*0.8)] < 7, Y[int(len(Y)*0.6):int(len(X)*0.8)], 0)\n",
    "Y_activities_test=np.where(Y[int(len(Y)*0.8):] < 7, Y[int(len(Y)*0.8):], 0)\n",
    "\n",
    "train_uid = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Train/subject_id_train.txt\")\n",
    "test_uid = np.loadtxt(\"/home/kchandrasekaran/wash/Datasets/uci-smartphone-based-recognition-of-human-activities/original/Test/subject_id_test.txt\")\n",
    "user_ids = np.hstack((train_uid, test_uid))\n",
    "rand_uid=[np.random.choice(np.unique(user_ids), len(np.unique(user_ids)), replace=False) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, labels):\n",
    "    Y_onehot=[]\n",
    "    for l in y:\n",
    "        empty_label=np.zeros(len(labels))\n",
    "        empty_label[labels.index(l)]=1.\n",
    "        Y_onehot.append(empty_label)\n",
    "    return(np.vstack(Y_onehot))\n",
    "\n",
    "\n",
    "def get_metrics(target, output):\n",
    "        \n",
    "        pred = onehot(np.argmax(output), range(target.shape[1]))\n",
    "        \n",
    "        tp = np.sum(((pred + target) == 2).astype(float), axis=0)\n",
    "        fp = np.sum(((pred - target) == 1).astype(float), axis=0)\n",
    "        fn = np.sum(((pred - target) == -1).astype(float), axis=0)\n",
    "        tn = np.sum(((pred + target) == 0).astype(float), axis=0)\n",
    "\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "        try:\n",
    "            prec = tp / (tp + fp)\n",
    "        except ZeroDivisionError:\n",
    "            prec = 0.0\n",
    "        try:\n",
    "            rec = tp / (tp + fn)\n",
    "        except ZeroDivisionError:\n",
    "            rec = 0.0\n",
    "        try:\n",
    "            specificity = tn / (tn + fp)\n",
    "        except ZeroDivisionError:\n",
    "            specificity = 0.0\n",
    "\n",
    "\n",
    "        try:\n",
    "            f1=2.*((prec*rec)/(prec+rec))\n",
    "        except ZeroDivisionError:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        acc[acc != acc] = 0.\n",
    "        prec[prec != prec] = 0.\n",
    "        rec[rec != rec] = 0.\n",
    "        specificity[specificity != specificity] = 0.\n",
    "        f1[f1 != f1] = 0.\n",
    "        \n",
    "        balanced_accuracy = (rec + specificity) / 2.\n",
    "        \n",
    "        err_rate = np.subtract(1., acc)\n",
    "        f1_micro, f1_macro, f1_weight, log_ls, roc = [], [], [], [], []\n",
    "        for idx in range(target.shape[1]):\n",
    "            y_test=target[:,idx]\n",
    "            y_pred=pred[:,idx]\n",
    "            \n",
    "            f1_micro.append(f1_score(y_test, y_pred, average= 'micro'))\n",
    "            f1_macro.append(f1_score(y_test, y_pred, average= 'macro'))\n",
    "            f1_weight.append(f1_score(y_test, y_pred, average= 'weighted'))\n",
    "            log_ls.append(log_loss(y_test, y_pred, labels=[0., 1.]))\n",
    "            try:\n",
    "                roc.append(roc_auc_score(y_test, output[:, idx]))\n",
    "            except ValueError:\n",
    "                roc.append(np.nan)\n",
    "            \n",
    "        return (balanced_accuracy, acc, err_rate, prec, rec, specificity, f1, tp, fp, fn, tn, np.array(f1_micro), np.array(f1_macro), np.array(f1_weight), np.array(log_ls), np.array(roc))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out =nn.Parameter(dtype(*size))\n",
    "    #torch.nn.init.xavier_normal(out)\n",
    "    torch.nn.init.zeros_(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_GRU(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Custom_GRU, self).__init__()\n",
    "        self.input_size = config[\"input_dim\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_layers = config[\"num_layers\"] \n",
    "        self.output_size = config[\"output_dim\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.num_epochs = config[\"num_epochs\"]\n",
    "        self.num_directions = config[\"num_directions\"]\n",
    "        self.attention_setting = config[\"attention\"]\n",
    "                \n",
    "        if config[\"num_directions\"] == 1:\n",
    "            bidirectional = False\n",
    "        elif config[\"num_directions\"] == 2:\n",
    "            bidirectional = True\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=self.input_size, num_layers= self.num_layers, hidden_size=self.hidden_size, \n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=0.1).cuda()\n",
    "        if(self.attention_setting == True):\n",
    "            self.linear = nn.Linear(self.hidden_size*self.num_directions, self.output_size)\n",
    "            self.attention_weights = new_parameter(1, self.hidden_size*self.num_directions)\n",
    "        else:\n",
    "            self.linear = nn.Linear(self.hidden_size*self.num_directions, self.output_size)\n",
    "        self.act = nn.Softmax()\n",
    "        #self.act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        if(self.attention_setting == True):\n",
    "            if(self.num_directions == 2):\n",
    "                H = torch.cat((hidden[0], hidden[1]), 1)\n",
    "            else:\n",
    "                H = hidden\n",
    "            M = F.tanh(H).squeeze()\n",
    "            alpha = self.act(torch.mm( M, torch.transpose(self.attention_weights, 1, 0)))\n",
    "            r = torch.mul(H , alpha.expand_as(H))\n",
    "            h_wa = F.tanh(r)\n",
    "            pred_out = self.act(self.linear(h_wa))\n",
    "        else:\n",
    "            pred_out = self.act(self.linear(pred)).unsqueeze(0).cuda()\n",
    "        \n",
    "        return pred_out\n",
    "\n",
    "    def train_gru(self, train_inp, train_out, test_inp, test_act_out, test_out):\n",
    "        predictions = []\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.learning_rate)\n",
    "        #attention_optimizer = torch.optim.Adam([self.attention_weights], lr=self.learning_rate, weight_decay=self.learning_rate)\n",
    "        #loss_func = nn.L1Loss()\n",
    "        loss_func = F.binary_cross_entropy\n",
    "        #attn_loss_fn = F.l1_loss\n",
    "        train_loss = [] \n",
    "        for t in range(self.num_epochs):\n",
    "            hidden = None\n",
    "            inp = Variable(torch.from_numpy(train_inp.reshape((train_inp.shape[0], -1, self.input_size))).type(dtype), requires_grad=True)\n",
    "            out = Variable(torch.from_numpy(train_out.reshape((train_inp.shape[0], self.output_size))).type(dtype))\n",
    "        \n",
    "            pred = self.forward(inp)\n",
    "            optimizer.zero_grad()\n",
    "            #attention_optimizer.zero_grad()\n",
    "            predictions.append(pred.data.cpu().numpy())\n",
    "            loss = loss_func(pred, out)\n",
    "            #attn_loss = attn_loss_fn(Variable(torch.gt(self.attention_weights, torch.zeros_like(self.attention_weights)).type(dtype), requires_grad=True), torch.zeros_like(self.attention_weights))\n",
    "            train_loss.append(loss.item())\n",
    "            if t%100==0:\n",
    "                print(t, loss.item())\n",
    "            loss.backward()\n",
    "            #attn_loss.backward()\n",
    "            optimizer.step()\n",
    "            #attention_optimizer.step()\n",
    "            \n",
    "        t_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, self.input_size))).type(dtype), requires_grad=True)\n",
    "        t_out = Variable(torch.Tensor(test_out.reshape((test_out.shape[0], self.output_size))).type(dtype))\n",
    "        pred_t = self.forward(t_inp)\n",
    "        test_loss = loss_func(pred_t, t_out)\n",
    "        pred_numpy = pred_t.squeeze().data.cpu().numpy()\n",
    "        \n",
    "        print(pred_numpy, test_act_out)\n",
    "        \n",
    "        filename= config[\"result_filename\"].replace(\"Transition\", \"Activities\")\n",
    "        \n",
    "        filename_modifier = 0\n",
    "        while(os.path.exists(\"{}_predictions_probabilities_target_{}\".format(filename, filename_modifier)+\".csv\")):\n",
    "            filename_modifier+=1\n",
    "        pred_fname=\"{}_predictions_probabilities_target_{}\".format(filename, filename_modifier)+\".csv\"\n",
    "        if(self.attention_setting == True):\n",
    "            with open(\"{}_attention_weights_{}\".format(filename, filename_modifier)+\".csv\", 'wb') as attn_file:\n",
    "                np.savetxt(attn_file, self.attention_weights.data.cpu().numpy(), delimiter=\",\")\n",
    "        \n",
    "        with open(pred_fname, 'a') as f:\n",
    "            #print(test_act_out.shape, pred_numpy.shape)\n",
    "            predictions=np.vstack((list(activity_label_names)*3,np.hstack((test_act_out, np.round(pred_numpy), pred_numpy))))\n",
    "            pd.DataFrame(predictions, columns=[\"Y\"]*(len(activity_label_names))+[\"Y_Hat\"]*(len(activity_label_names))+[\"Y_Hat_Prob\"]*(len(activity_label_names))).to_csv(f, index=False)\n",
    "        \n",
    "        results_metrics = get_metrics(test_act_out, pred_numpy)\n",
    "        results_conf_mat = confusion_matrix(np.argmax(test_act_out, axis=1)+1, np.argmax(pred_numpy, axis=1)+1)\n",
    "        metric_names = np.array([\"CV\", \"Balanced Accuracy\", \"Accuracy\", \"Error Rate\", \"Precision\",\"Recall\",\"Specificity\", \"F1\", \"TP\",\"FP\",\"FN\",\"TN\", \"Micro F1\",\"Macro F1\",\"Weighted F1\",\"Log-Loss\",\"ROC AUC\"])\n",
    "        results = np.hstack((metric_names.reshape(-1, 1), np.vstack((activity_label_names, np.vstack(results_metrics)))))\n",
    "\n",
    "        with open(\"{}.csv\".format(filename), 'a') as f:\n",
    "            pd.DataFrame(results).to_csv(f, header=False)\n",
    "        with open(\"{}_conf_matrix.csv\".format(filename), 'a') as f:    \n",
    "            pd.DataFrame(np.hstack((activity_label_names.reshape(-1, 1), results_conf_mat))).to_csv(f, header=False)\n",
    "\n",
    "        transition_act=np.where(np.argmax(pred_numpy, axis=1)==6, True, False)\n",
    "        #print(transition_act)\n",
    "        groups = [(list(v), g) for g,v in itertools.groupby(transition_act)]\n",
    "        processed=0\n",
    "        transition_y_hat=[]\n",
    "        #print(groups)\n",
    "        for group in groups:\n",
    "            cont=group[0]\n",
    "            g=group[1]\n",
    "            trans_pred=0\n",
    "            if g:\n",
    "                if(len(cont)>0):\n",
    "                    prev_act=np.argmax(pred_numpy[processed-1])+1\n",
    "                    next_act=np.argmax(pred_numpy[processed+len(cont)])+1\n",
    "                    if(prev_act==5 and next_act==4):\n",
    "                        trans_pred=7\n",
    "                    elif(prev_act==5 and next_act==6):\n",
    "                        trans_pred=11\n",
    "                    elif(prev_act==4 and next_act==5):\n",
    "                        trans_pred=8\n",
    "                    elif(prev_act==4 and next_act==6):\n",
    "                        trans_pred=9\n",
    "                    elif(prev_act==6 and next_act==4):\n",
    "                        trans_pred=10\n",
    "                    elif(prev_act==6 and next_act==5):\n",
    "                        #print(\"class 12 should be predicted\")\n",
    "                        trans_pred=12\n",
    "                    elif(prev_act==6 and next_act==1):\n",
    "                        trans_pred=12\n",
    "                    else:\n",
    "                        print(\"Unexpected Combination. Prev{}. Next{}\".format(prev_act, next_act))\n",
    "                    processed+=len(cont)\n",
    "                else:\n",
    "                    processed+=len(cont)\n",
    "            else:\n",
    "                processed+=len(cont)\n",
    "            transition_y_hat.append(np.ones_like(cont)*trans_pred)\n",
    "        transition_y_hat = np.hstack(transition_y_hat)\n",
    "        \n",
    "        pred_one_hot = one_hot(transition_y_hat, [0, 7, 8, 9, 10, 11, 12])\n",
    "        \n",
    "        results_metrics = get_metrics(test_out, pred_one_hot)\n",
    "        results_conf_mat = confusion_matrix(np.argmax(test_out, axis=1)+1, np.argmax(pred_one_hot, axis=1)+1)\n",
    "        metric_names = np.array([\"CV\", \"Balanced Accuracy\", \"Accuracy\", \"Error Rate\", \"Precision\",\"Recall\",\"Specificity\", \"F1\", \"TP\",\"FP\",\"FN\",\"TN\", \"Micro F1\",\"Macro F1\",\"Weighted F1\",\"Log-Loss\",\"ROC AUC\"])\n",
    "        results = np.hstack((metric_names.reshape(-1, 1), np.vstack((transition_label_names, np.vstack(results_metrics)))))\n",
    "        \n",
    "        filename= config[\"result_filename\"]\n",
    "        \n",
    "        with open(\"{}.csv\".format(filename), 'a') as f:\n",
    "            pd.DataFrame(results).to_csv(f, header=False)\n",
    "        with open(\"{}_conf_matrix.csv\".format(filename), 'a') as f:    \n",
    "            pd.DataFrame(np.hstack((transition_label_names.reshape(-1, 1), results_conf_mat))).to_csv(f, header=False)\n",
    "            \n",
    "        filename_modifier = 0\n",
    "        while(os.path.exists(\"{}_predictions_probabilities_target_{}\".format(filename, filename_modifier)+\".csv\")):\n",
    "            filename_modifier+=1\n",
    "        pred_fname=\"{}_predictions_probabilities_target_{}\".format(filename, filename_modifier)+\".csv\"\n",
    "        \n",
    "        #print(test_out.shape, pred_one_hot.shape)\n",
    "        with open(pred_fname, 'a') as f:\n",
    "            predictions=np.vstack((list(transition_label_names)*2,np.hstack((test_out, pred_one_hot))))\n",
    "            pd.DataFrame(predictions, columns=[\"Y\"]*(len(transition_label_names))+[\"Y_Hat\"]*(len(transition_label_names))).to_csv(f, index=False)\n",
    "      \n",
    "        plt.subplots(figsize=(20,15))\n",
    "        sns.set(font_scale = 1.8)\n",
    "        s=sns.heatmap(results_conf_mat.astype(int), annot=True, annot_kws={\"size\": 20}, cmap=\"YlGnBu\", fmt='d', xticklabels=transition_label_names, yticklabels=transition_label_names)\n",
    "        title=\"Transition Learning\"\n",
    "        s.set_title(title)\n",
    "        \n",
    "        filename_modifier = 0\n",
    "        while(os.path.exists(filename+\"_\"+str(filename_modifier)+\".png\")):\n",
    "            filename_modifier+=1\n",
    "        fig_fname=filename+\"_\"+str(filename_modifier)+\".png\"\n",
    "        s.get_figure().savefig(fig_fname, dpi=400)\n",
    "        plt.close()\n",
    "        \n",
    "        with open(filename+\"_\"+str(filename_modifier)+\"_train_loss.csv\", 'wb') as train_loss_file:\n",
    "            wr = csv.writer(train_loss_file, quoting=csv.QUOTE_ALL)\n",
    "            for tr_l in train_loss:\n",
    "                wr.writerow([tr_l])\n",
    "        final_loss = test_loss.detach().cpu().numpy().item()\n",
    "        del inp\n",
    "        del self.rnn\n",
    "        del out\n",
    "        del t_inp\n",
    "        del t_out\n",
    "        del test_loss\n",
    "        del pred\n",
    "        del pred_t\n",
    "        del train_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        plt.close()\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "hidden_layer_sizes=[1024]\n",
    "learning_rates=[1e-3]\n",
    "epochs=[5000]\n",
    "directions = [1]\n",
    "layers=[1]\n",
    "attention = [True]\n",
    "    \n",
    "hyperparameters = [hidden_layer_sizes, directions, layers, epochs, learning_rates, attention]\n",
    "all_parameter_combinations=list(itertools.product(*hyperparameters))\n",
    "costs=[]\n",
    "for parameter_combo in all_parameter_combinations:\n",
    "    config = {}\n",
    "    config[\"input_dim\"] = 561\n",
    "    config[\"hidden_size\"] = parameter_combo[0] \n",
    "    config[\"num_directions\"] = parameter_combo[1]\n",
    "    config[\"num_layers\"] = parameter_combo[2]\n",
    "    config[\"output_dim\"] = 7\n",
    "    config[\"num_epochs\"] = parameter_combo[3]\n",
    "    config[\"learning_rate\"] = parameter_combo[4]\n",
    "    config[\"attention\"] = parameter_combo[5]\n",
    "    if(config[\"attention\"] == True):\n",
    "        config[\"result_filename\"] = \"results/attention_decay/No_Decay_Inverse_Transition_classification_w_attention_results_BiGRU_{}_directions_{}_layers_{}_lr_{}_units_{}_epochs\".format(config[\"num_directions\"], config[\"num_layers\"], config[\"learning_rate\"], config[\"hidden_size\"], config[\"num_epochs\"])\n",
    "    else:\n",
    "        config[\"result_filename\"] = \"results/attention_decay/No_Decay_Inverse_Transition_classification_results_BiGRU_{}_directions_{}_layers_{}_lr_{}_units_{}_epochs\".format(config[\"num_directions\"], config[\"num_layers\"], config[\"learning_rate\"], config[\"hidden_size\"], config[\"num_epochs\"])\n",
    "    for cv_idx, cv_fold in enumerate(rand_uid):\n",
    "        train_ids, val_ids, test_ids = cv_fold[:int(0.6*len(cv_fold))], cv_fold[int(0.6*len(cv_fold)):int(0.8*len(cv_fold)):], cv_fold[int(0.8*len(cv_fold)):]\n",
    "        train_idx = np.isin(user_ids, train_ids)\n",
    "        val_idx = np.isin(user_ids, val_ids)\n",
    "        test_idx = np.isin(user_ids, test_ids)\n",
    "        \n",
    "        X_activity_train=X[train_idx]\n",
    "        X_activity_validation=X[val_idx]\n",
    "        X_activity_test=X[test_idx]\n",
    "        Y_activity_train=np.where(Y[train_idx] <= 6, Y[train_idx], 0)\n",
    "        Y_activity_validation=np.where(Y[val_idx] <= 6, Y[val_idx], 0)\n",
    "        Y_activity_test=np.where(Y[test_idx] <= 6, Y[test_idx], 0)\n",
    "        \n",
    "        Y_transition_train=np.where(Y[train_idx] > 6, Y[train_idx], 0)\n",
    "        Y_transition_validation=np.where(Y[val_idx] > 6, Y[val_idx], 0)\n",
    "        Y_transition_test=np.where(Y[test_idx] > 6, Y[test_idx], 0)\n",
    "        \n",
    "        train_act_inp, train_act_out = X_activity_train, one_hot(Y_activity_train, [1, 2, 3, 4, 5, 6, 0])\n",
    "        val_act_inp, val_act_out = X_activity_validation, one_hot(Y_activity_validation, [1, 2, 3, 4, 5, 6, 0])\n",
    "        test_act_inp, test_act_out = X_activity_test, one_hot(Y_activity_test, [1, 2, 3, 4, 5, 6, 0])\n",
    "            \n",
    "        train_trans_inp, train_trans_out = X_activity_train, one_hot(Y_transition_train, [0,7,8,9,10,11,12])\n",
    "        val_trans_inp, val_trans_out = X_activity_validation, one_hot(Y_transition_validation, [0,7,8,9,10,11,12])\n",
    "        test_trans_inp, test_trans_out = X_activity_test, one_hot(Y_transition_test, [0,7,8,9,10,11,12])\n",
    "        \n",
    "        print(val_act_out)\n",
    "        break\n",
    "        start_time = time.time()\n",
    "        model = Custom_GRU(config).to(device)\n",
    "        loss = model.train_gru(train_act_inp, train_act_out, val_act_inp, val_act_out, val_trans_out)\n",
    "        costs.append(loss)\n",
    "        config[\"CV_index\"] = cv_idx\n",
    "        config[\"test_loss\"] = loss\n",
    "        config[\"time_elapsed\"] = time.time()-start_time\n",
    "        with open(\"Inverse_GRU_W_New_attention_testing.csv\", 'a') as f:    \n",
    "            #print(config)\n",
    "            pd.DataFrame(config, index=[0]).to_csv(f, header=False)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    #best_params=all_parameter_combinations[np.argmin(costs)]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
